/* ***** BEGIN LICENSE BLOCK *****
 * Version: MPL 1.1/GPL 2.0/LGPL 2.1
 *
 * The contents of this file are subject to the Mozilla Public License Version
 * 1.1 (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 * http://www.mozilla.org/MPL/
 *
 * Software distributed under the License is distributed on an "AS IS" basis,
 * WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
 * for the specific language governing rights and limitations under the
 * License.
 *
 * The Original Code is Ajax.org Code Editor (ACE).
 *
 * The Initial Developer of the Original Code is
 * Ajax.org Services B.V.
 * Portions created by the Initial Developer are Copyright (C) 2010
 * the Initial Developer. All Rights Reserved.
 *
 * Contributor(s):
 *   Fabian Jakobs <fabian AT ajax DOT org>
 *   Andrew Sutherland <asutherland@asutherland.org>
 *
 * Alternatively, the contents of this file may be used under the terms of
 * either the GNU General Public License Version 2 or later (the "GPL"), or
 * the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
 * in which case the provisions of the GPL or the LGPL are applicable instead
 * of those above. If you wish to allow use of your version of this file only
 * under the terms of either the GPL or the LGPL, and not to allow others to
 * use your version of this file under the terms of the MPL, indicate your
 * decision by deleting the provisions above and replace them with the notice
 * and other provisions required by the GPL or the LGPL. If you do not delete
 * the provisions above, a recipient may use your version of this file under
 * the terms of any one of the MPL, the GPL or the LGPL.
 *
 * ***** END LICENSE BLOCK ***** */

/**
 * This is a modified version of ace's background_tokenizer.js changed to
 *  support tokenizers that operate on the entirety of a program's source
 *  at once.  The basic idea is that we tell aware tokenizers about all the
 *  lines immediately prior to trying to get any answers out of them.
 *
 * I have also added some comments.
 **/

define(function(require, exports, module) {

var oop = require("pilot/oop");
var EventEmitter = require("pilot/event_emitter").EventEmitter;

/**
 * @typedef[TokenizedLine @dict[
 *   @key[tokens @listof[Token]]
 *   @key[state String]{
 *     Terminal state for the line.  Only matters for (otherwise) stateless
 *     line-based tokenizers.
 *   }
 *
 * ]]{
 *
 * }
 **/

/**
 * Support the jstut use-case by support parsing tokenizers that parse the
 *  document in its entirety with fallbacks to line-based tokenizers when
 *  the whole document parser falls down.  We also currently always defer
 *  to the line-based tokenizer for all editor indentation functionality and
 *  the like.
 *
 * The basic idea is that if the parser is able to parse the entire document,
 *  then we use it as the source of tokens.  If the parser chokes on the
 *  document, we use the parts it is able to provide us with, otherwise failing
 *  over to the line-based tokenizer for whatever it cannot provide us with.
 *
 * Our tokenized line status is accordingly stored in two tiers: `parsedLines`
 *  holds the lines as generated by the parser while `fallbackLines` holds the
 *  lines as generated by the fallback tokenizer.  We null out the `parsedLines`
 *  that become invalidated and use the fallback lines in that case.  Because
 *  the Document object very quickly starts generating unbounded update ranges
 *  (where lastRow === undefined) in complex cases, we don't need to deal with
 *  complex insertion/deletion scenarios.
 */
var GlobalFallbackTokenizer = function(tokenizer, editor) {
    this.running = false;
    /**
     * How many lines in `doc` last time we checked?  Lets us detect
     *  deltas (which we are not explicitly told about).
     */
    this.knownLineCount = 0;

    /** @listof[TokenizedLine] */
    this.parsedLines = [];
    /** @listof[TokenizedLine] */
    this.fallbackLines = [];
    /** the next line to be processed; equal to doc's length when done */
    this.currentLine = 0;

    this._setTokenizer(tokenizer);

    // XXX debug inspection helper; no functional purpose
    //window.tokenizer = this;

    var self = this;

    // A self-rescheduling timeout-based driver that processes as many sets of
    //  5 lines within >= 20ms as it can.  It reschedules itself ever 20 ms.
    this.$worker = function() {
        if (!self.running) { return; }

        var workerStart = new Date();
        var startLine = self.currentLine;
        var doc = self.doc;

        var processedLines = 0;

        var len = doc.getLength();
        while (self.currentLine < len) {
            self.fallbackLines[self.currentLine] =
              self.$tokenizeRows(self.currentLine, self.currentLine, false)[0];
            self.currentLine++;

            // only check every 5 lines
            processedLines += 1;
            if ((processedLines % 5 == 0) && (new Date() - workerStart) > 20) {
                self.fireUpdateEvent(startLine, self.currentLine-1);

                self.running = setTimeout(self.$worker, 20);
                return;
            }
        }

        self.running = false;

        self.fireUpdateEvent(startLine, len - 1);
    };
};

(function(){

    oop.implement(this, EventEmitter);

    this._setTokenizer = function(tokenizer) {
      if (tokenizer &&
          ("wholeDocumentParser" in tokenizer) &&
          tokenizer.wholeDocumentParser) {
        this.parser = tokenizer;
        this.tokenizer = tokenizer.fallbackTokenizer;
      }
      else {
        this.parser = null;
        this.tokenizer = tokenizer;
      }
      this.parserDirty = false;
    },

    this.setTokenizer = function(tokenizer) {
        this._setTokenizer(tokenizer);
        this.parsedLines = [];
        this.fallbackLines = [];

        this.start(0);
    };

    this.setDocument = function(doc) {
        this.doc = doc;
        this.knownLineCount = doc.getLength();
        this.parsedLines = [];
        this.fallbackLines = [];

        this.stop();
    };

    /**
     * Tell listeners about our recently tokenized lines.
     */
    this.fireUpdateEvent = function(firstRow, lastRow) {
        var data = {
            first: firstRow,
            last: lastRow
        };
        console.log("tokenizer saying invalidating", firstRow, lastRow);
        this._dispatchEvent("update", {data: data});
    };

    /**
     * Notifies us that the document has changed at line `startRow` through
     *  `lastRow`.  Line based tokenizers effectively invalidate from startRow
     *  on.
     *
     * XXX And in actuality, it appears all invalidations are always
     *  through the last row.  We used to try and thunk the document change
     *  event to get a value for lastRow passed through to us.  Things changed
     *  so that now we would want to thunk EditSession.onChange to pass the
     *  value through to us, except that it would make no difference.
     */
    this.start = function(startRow, lastRow) {
        if (this.parser) {
          this.parserDirty = true;
          if (lastRow === undefined) {
            // unbounded nukes make the parser forget about everything.
            this.parsedLines.splice(startRow, this.fallbackLines.length);
            this.parser.nukeState();
          }
          else {
            // net changes in document lines insert or remove nulls at startRow
            var delta = this.doc.getLength() - this.knownLineCount;

            if (delta || lastRow - startRow > 0)
              this.parser.nukeState();
            else
              this.parser.lineChanged(startRow,
                                      this.doc.getLines(startRow, startRow));

            if (delta > 0) {
              while (delta--)
                this.parsedLines.splice(startRow, 0, null);
            }
            else {
              this.parsedLines.splice(startRow, -delta);
            }
            this.knownLineCount = delta;

            // constrained variations just null out the affected lines so we
            //  look through them
            for (var i = startRow; i <= lastRow; i++) {
              this.parsedLines[i] = null;
            }

          }
        }

        this.currentLine = Math.min(startRow || 0, this.currentLine,
                                    this.doc.getLength());

        // remove all cached items below this line
        this.fallbackLines.splice(this.currentLine, this.fallbackLines.length);

        this.stop();
        // pretty long delay to prevent the tokenizer from interfering with the user
        this.running = setTimeout(this.$worker, 700);
    };

    this.stop = function() {
        if (this.running)
            clearTimeout(this.running);
        this.running = false;
    };

    this.getTokens = function(firstRow, lastRow) {
        //console.log("getting asked for", firstRow, lastRow);
        return this.$tokenizeRows(firstRow, lastRow, true);
    };

    this.getState = function(row) {
        return this.$tokenizeRows(row, row, false)[0].state;
    };

    this.$tokenizeRows = function(firstRow, lastRow, allowParser) {
        if (!this.doc || isNaN(firstRow) || isNaN(lastRow))
            return [{'state':'start','tokens':[]}];

        var rows = [];
        var sourceString = "";

        if (this.parser) {
          if (this.parserDirty) {
            var parsed = this.parser.parse(this.doc.getAllLines());
            console.log("parser says it parsed up through",
                        parsed.firstBadLine);
            var parsedLines = this.parsedLines = [];
            for (var i = 0; i < parsed.firstBadLine; i++) {
              parsedLines.push(this.parser.getLineTokens(i));
            }
            this.parserDirty = false;
          }
        }

        // determine start state
        var state = "start";
        var doCache = false;
        if (firstRow > 0 && this.fallbackLines[firstRow - 1]) {
            state = this.fallbackLines[firstRow - 1].state;
            doCache = true;
        }

        var lines = this.doc.getLines(firstRow, lastRow);
        for (var row=firstRow; row<=lastRow; row++) {
            var rowNeeded = true;
            if (allowParser &&
                this.parsedLines.length > row &&
                this.parsedLines[row]) {
              rows.push(this.parsedLines[row]);
              rowNeeded = false;
              sourceString += "P";
            }
            else {
              sourceString += "f";
            }
            if (!this.fallbackLines[row]) {
                var tokens = this.tokenizer.getLineTokens(lines[row-firstRow] || "", state);
                state = tokens.state;
                if (rowNeeded)
                  rows.push(tokens);

                if (doCache) {
                    this.fallbackLines[row] = tokens;
                }
            }
            else {
                var tokens = this.fallbackLines[row];
                state = tokens.state;
                if (rowNeeded)
                  rows.push(tokens);
            }
        }
        if (allowParser)
          console.log("source:", sourceString);
        return rows;
    };

}).call(GlobalFallbackTokenizer.prototype);

exports.GlobalFallbackTokenizer = GlobalFallbackTokenizer;
});
